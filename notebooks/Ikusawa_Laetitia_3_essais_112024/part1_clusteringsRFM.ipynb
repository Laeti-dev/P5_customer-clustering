{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from math import pi\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = pd.read_pickle('../../data/customers.pkl')\n",
    "df_customers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start our segmentation, we will simply follow the RFM marketing method that consists in categorize customers according 3 main features :\n",
    "- recency : when was the last order\n",
    "- frequency : how often customers order\n",
    "- monetary : how much a costumor can spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm = df_customers[\n",
    "    [\n",
    "        \"customer_unique_id\",\n",
    "        \"days_since_last_order\", # recency\n",
    "        \"total_orders\", # frequency\n",
    "        \"total_expenses\", # monetary\n",
    "    ]\n",
    "]\n",
    "\n",
    "df_rfm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 0 days by 1 day to avoid having 0\n",
    "df_rfm['days_since_last_order'] = df_rfm[\"days_since_last_order\"].apply(lambda x: 1 if x == 0 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring (The idea was to libellised groups but we did not continue this way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recency score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the last order date distribution showing cde\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.displot(df_rfm[\"days_since_last_order\"], kde=True)\n",
    "plt.axvline(\n",
    "    df_rfm[\"days_since_last_order\"].mean(), color=\"red\", linestyle=\"--\", label=\"Mean\"\n",
    ")\n",
    "plt.axvline(\n",
    "    df_rfm[\"days_since_last_order\"].median(),\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Median\",\n",
    ")\n",
    "plt.title('Days Since Last Order Distribution')\n",
    "plt.xlabel('Days Since Last Order')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot the distribution of days since last order\n",
    "fig = px.box(df_rfm, x=\"days_since_last_order\", title='Distribution of Days Since Last Order', width=800, height=400)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column that categorizes the days since last order according to the quantiles.\n",
    "\n",
    "# Define the quantiles\n",
    "quantiles = df_rfm['days_since_last_order'].quantile(q=[0.25, 0.5, 0.75, 1])\n",
    "\n",
    "# Create a function that assigns a category to each customer based on the quantiles\n",
    "def categorize_days_since_last_order(days):\n",
    "    if days <= quantiles[0.25]:\n",
    "        return 5\n",
    "    elif days <= quantiles[0.5]:\n",
    "        return 4\n",
    "    elif days <= quantiles[0.75]:\n",
    "        return 3\n",
    "    elif days <= quantiles[1]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Apply the function to the days_since_last_order column\n",
    "df_rfm['recency_score'] = df_rfm['days_since_last_order'].apply(categorize_days_since_last_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of total orders\n",
    "fig =  px.histogram(df_rfm, x='total_orders', title='Distribution of Recency Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each unique value in the total_orders columns, add the % of customers that fall into each category\n",
    "df_rfm['total_orders'].value_counts(normalize=True) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97% of customers did a unique order. Calculate a scoreon the frequencies does not give us a valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers order at least one time and 16 times at the most. How is it distributed ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monetary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm[\"total_expenses\"] = df_rfm[\"total_orders_value\"] + df_rfm[\"total_freight_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the total expenses distribution\n",
    "group_labels = [\"Order value\", \"Freight value\"]\n",
    "\n",
    "fig = ff.create_distplot([df_rfm[\"total_orders_value\"], df_rfm[\"total_freight_value\"]], group_labels, bin_size=1000, curve_type='kde')\n",
    "\n",
    "# Overlay histograms\n",
    "fig.update_layout(title='Distribution of Total Expenses', barmode='overlay')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot the distribution of total expenses\n",
    "fig = px.box(df_rfm, x='total_expenses', title='Distribution of Total Expenses')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = np.quantile(df_rfm[\"total_expenses\"], 0.25)\n",
    "Q3 = np.quantile(df_rfm[\"total_expenses\"], 0.75)\n",
    "IQR = Q3 - Q1\n",
    "expenses_distribution = df_rfm[~(df_rfm[\"total_expenses\"] > Q3 + (1.5 * IQR))]\n",
    "expenses_distribution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(expenses_distribution, x='total_expenses', title='Distribution of Total Expenses', width=800, height=400, marginal='box')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a monetary score column\n",
    "quantiles = df_rfm['total_expenses'].quantile(q=[0.25, 0.5, 0.75, 1])\n",
    "\n",
    "def categorize_total_expenses(expenses):\n",
    "    if expenses <= quantiles[0.25]:\n",
    "        return 1\n",
    "    elif expenses <= quantiles[0.5]:\n",
    "        return 2\n",
    "    elif expenses <= quantiles[0.75]:\n",
    "        return 3\n",
    "    elif expenses <= quantiles[1]:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "df_rfm['monetary_score'] = df_rfm['total_expenses'].apply(categorize_total_expenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work on the features thats belongs to RFM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_rfms = df_rfm[[\"customer_unique_id\",\"days_since_last_order\",'recency_score', 'total_orders', 'monetary_score', \"total_expenses\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took in consideration 2 scores : Recency and monetary. \n",
    "Let's create a RFM_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_rfms[\"rfm_score\"] = (main_rfms[\"recency_score\"] + main_rfms[\"monetary_score\"]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_rfms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a scatter plot of expenses per customer\n",
    "fig = px.scatter(\n",
    "    main_rfms,\n",
    "    y='days_since_last_order',\n",
    "    x='total_expenses',\n",
    "    color='rfm_score',\n",
    "    title='3D Scatter Plot of Expenses per Customer',\n",
    "    labels={'total_orders_value': 'Total Orders Value', 'total_freight_value': 'Total Freight Value', 'total_expenses': 'Total Expenses'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "def scale_data_and_plot_distributions(original_df: pd.DataFrame, features: list[str], plot: bool = True):\n",
    "    # Apply logarithm transformation\n",
    "    df = original_df[features].copy()\n",
    "    log_df = df.copy()\n",
    "    log_df = df.apply(np.log1p)\n",
    "\n",
    "    fig, axes = plt.subplots(len(features), 3, figsize=(20, 12))\n",
    "\n",
    "    if plot:\n",
    "        for i, feature in enumerate(original_df.columns):\n",
    "            sns.histplot(original_df[features], kde=True, ax=axes[i, 0])\n",
    "            axes[i, 0].set_title(f\"{feature} original distribution\")\n",
    "            axes[i, 0].set_xlabel(\"Value\")\n",
    "            axes[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "            sns.histplot(log_df[feature], kde=True, ax=axes[i, 1])\n",
    "            axes[i, 1].set_title(f\"{feature} log distribution\")\n",
    "            axes[i, 1].set_xlabel(\"Value\")\n",
    "            axes[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return df, log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_clusters(df: pd.DataFrame, algorithm: str=\"KMeans\"):\n",
    "    \"\"\"Plots a 3D scatter plot of the RFM clusters.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Scalerd dataframe with cluster labels\n",
    "    \"\"\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for each cluster\n",
    "    for cluster in sorted(df[\"cluster\"].unique()):\n",
    "        cluster_data = df[df[\"cluster\"] == cluster]\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=cluster_data[cluster_data.columns[0]],\n",
    "                y=cluster_data[cluster_data.columns[1]],\n",
    "                z=cluster_data[cluster_data.columns[2]],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=4),\n",
    "                name=f\"Cluster {cluster}\",\n",
    "                text=cluster_data.index,  # Add index as text for annotation\n",
    "                hoverinfo=\"text+x+y+z\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Clusters 3D representation for {algorithm} with {len(df[\"cluster\"].unique())} clusters and {len(df.columns.to_list())-1} features\",\n",
    "        scene=dict(\n",
    "            xaxis_title=df.columns[0],\n",
    "            yaxis_title=df.columns[1],\n",
    "            zaxis_title=df.columns[2],\n",
    "        ),\n",
    "        legend=dict(title=\"Clusters\"),\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a radar chart of the clusters\n",
    "def plot_radar_chart(df: pd.DataFrame):\n",
    "    \"\"\"Plots a radar chart of the clusters.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Scaled dataframe\n",
    "    \"\"\"\n",
    "    # Calculate the mean of each feature for each cluster\n",
    "    cluster_means = df.groupby(\"cluster\").mean()\n",
    "\n",
    "    # Normalize the cluster means\n",
    "    scaler = StandardScaler()\n",
    "    cluster_means_normalized = pd.DataFrame(\n",
    "        scaler.fit_transform(cluster_means), columns=cluster_means.columns\n",
    "    )\n",
    "\n",
    "    # Number of variables we're plotting\n",
    "    categories = list(cluster_means_normalized.columns)\n",
    "    N = len(categories)\n",
    "\n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variables)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    # Initialise the spider plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "    # Loop through each cluster and plot\n",
    "    for i, row in cluster_means_normalized.iterrows():\n",
    "        values = row.tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=f'Cluster {i}')\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "    # Add labels\n",
    "    plt.xticks(angles[:-1], categories, color=\"grey\", size=8)\n",
    "\n",
    "    # Add a title and legend\n",
    "    plt.title(\"Radar Chart for Clusters\", size=20, color=\"black\", y=1.1)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_clusters(original_df: pd.DataFrame, add_cols: list[str]=None) -> pd.DataFrame:\n",
    "    cols = [\"days_since_last_order\", \"total_orders\", \"total_expenses\"]\n",
    "    if add_cols:\n",
    "        cols += add_cols\n",
    "\n",
    "    # original_df = original[cols].copy()\n",
    "    # original_df[\"cluster\"] = df[\"cluster\"]\n",
    "\n",
    "    agg_functions = {\n",
    "        \"cluster\":[\"count\", lambda x : round(x.count() / original_df.shape[0] * 100, 2)],\n",
    "        \"days_since_last_order\": lambda x: round(x.mean(),2),\n",
    "        \"total_orders\": \"mean\",\n",
    "        \"total_expenses\": [lambda x: round(x.mean(), 2), \"sum\", lambda x: round((x.sum() / (original_df[\"total_expenses\"].sum())) * 100, 2)]\n",
    "    }\n",
    "\n",
    "    clusters_summary = original_df.groupby(\"cluster\").agg(agg_functions).reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    clusters_summary.columns = [\n",
    "        \"_\".join(col).strip() if isinstance(col, tuple) else col\n",
    "        for col in clusters_summary.columns.values\n",
    "    ]\n",
    "\n",
    "    # Rename the columns\n",
    "    clusters_summary = clusters_summary.rename(\n",
    "        columns={\n",
    "            \"clusters_\" : \"cluster\",\n",
    "            \"cluster_count\": \"customers\",\n",
    "            \"cluster_<lambda_0>\": \"customers(%)\",\n",
    "            \"total_orders_mean\": \"average_orders\",\n",
    "            \"total_expenses_<lambda_0>\": \"average_basket(real)\",\n",
    "            \"total_expenses_sum\": \"total_revenues(real)\",\n",
    "            \"total_expenses_<lambda_1>\": \"total_revenues(%)\",\n",
    "            \"days_since_last_order_<lambda>\": \"average_recency\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    display(clusters_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(combinations, df):\n",
    "    scores = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (eps, min_samples) in enumerate(combinations):\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(df)\n",
    "        labels = dbscan.labels_\n",
    "        labels_set = set(labels)\n",
    "        num_clusters = len(labels_set)\n",
    "        if -1 in labels_set:\n",
    "            num_clusters -= 1\n",
    "        all_labels.append(labels)\n",
    "        if (num_clusters < 2) or (num_clusters > 10):\n",
    "            scores.append(-20)\n",
    "            all_labels.append(\"Poor\")\n",
    "            print(f\"Combination {i} - eps: {eps}, min_samples: {min_samples}, number of clusters: {num_clusters}\")\n",
    "            continue\n",
    "        score = silhouette_score(df, labels)\n",
    "\n",
    "        scores.append(score)\n",
    "        all_labels.append(labels)\n",
    "        print(f\"Combination {i} - eps: {eps}, min_samples: {min_samples}, number of clusters: {num_clusters}, score: {score}\")\n",
    "\n",
    "    best_index = np.argmax(scores)\n",
    "    best_params = combinations[best_index]\n",
    "    best_labels = all_labels[best_index]\n",
    "    best_score = scores[best_index]\n",
    "\n",
    "    return {\"epsylon\": best_params[0], \"min_samples\": best_params[1], \"score\": best_score, \"labels\": best_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHC gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ahc_best_params(sample: pd.DataFrame, distance_measures: list[str], linkage_methods: list[str]):\n",
    "    \"\"\"Gets the best params for the CHA method.\n",
    "\n",
    "    Args:\n",
    "        sample (pd.DataFrame): dataframe to be used\n",
    "        distance_measures (list[str]):  list of distances between individuals\n",
    "        linkage_methods (list[str]): List of methods to calculate distances between clusters\n",
    "    \"\"\"\n",
    "    # Find optimal distances between individuals and clusters\n",
    "    scores_dict = {}\n",
    "\n",
    "    # Iterate over the distance measures to find the best silhouette score\n",
    "    for distance in distance_measures:\n",
    "        for linkage_method in linkage_methods:\n",
    "            # Create the linkage matrix\n",
    "            if (linkage_method == \"ward\" or linkage_method == \"centroid\") and distance != \"euclidean\":\n",
    "                # distance = \"euclidean\"\n",
    "                # Z = linkage(sample, method=linkage_method, metric=distance)\n",
    "                continue\n",
    "            # else:\n",
    "            Z = linkage(sample, method=linkage_method, metric=distance)\n",
    "\n",
    "            # Set a distance threshold\n",
    "            # max_d =\n",
    "            # clusters = fcluster(Z, max_d, criterion=\"distance\")\n",
    "            max_clusters = 10\n",
    "\n",
    "            best_score = -1\n",
    "\n",
    "\n",
    "            max_clusters = 10\n",
    "\n",
    "            for t in np.linspace(0, np.max(Z[:, 2]), 100):\n",
    "                clusters = fcluster(Z, t, criterion=\"distance\")\n",
    "                if len(set(clusters)) > 1 and len(set(clusters)) <= max_clusters:\n",
    "                    score = silhouette_score(sample, clusters)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_threshold = t\n",
    "\n",
    "            scores_dict[(distance, linkage_method)] = best_score\n",
    "            print(f\"Distance: {distance}, Linkage: {linkage_method}, Score: {best_score}\")\n",
    "                        # # Calculate the silhouette score\n",
    "                        # if len(set(clusters)) > 1:\n",
    "                        #     average_score = silhouette_score(sample, clusters, metric=distance)\n",
    "                        #     scores_dict[(distance, linkage_method)] = average_score\n",
    "                        #     print(f\"Distance: {distance}, Linkage: {linkage_method}, Score: {average_score}\")\n",
    "\n",
    "    # Get the best distance measure and linkage method\n",
    "    best_params = max(scores_dict, key=scores_dict.get)\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the df_rfm distribution for each feature\n",
    "fig, axes = plt.subplots(3, 1, figsize=(20, 12))\n",
    "\n",
    "for i, feature in enumerate(df_rfm.columns[1:]):\n",
    "    sns.histplot(df_rfm[feature], kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} Distribution')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current working dataset, the selected features are numericals. However, we are dealing with numbers of orders, number of days and expenses amount, which means different scales. To start, we will use the StandardScaler algorithm that will transform data so that it has a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rfm.set_index('customer_unique_id', inplace=True)\n",
    "df_rfm.drop(columns=[\"customer_unique_id\"], inplace=True)\n",
    "\n",
    "# Initialize sclaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_features = scaler.fit_transform(df_rfm)\n",
    "\n",
    "# Create a dataframe from the scaled features\n",
    "scaled_df_rfm = pd.DataFrame(scaled_features, columns=df_rfm.columns)\n",
    "\n",
    "scaled_df_rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaled_features = minmax_scaler.fit_transform(df_rfm)\n",
    "\n",
    "# Create a dataframe from the scaled features\n",
    "minmax_scaled_df_rfm = pd.DataFrame(minmax_scaled_features, columns=df_rfm.columns)\n",
    "\n",
    "minmax_scaled_df_rfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log before normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also will try to apply the logarithm before scaling as the distribution can be asymetrical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_values = (df_rfm == 0).sum()\n",
    "neg_values = (df_rfm < 0).sum()\n",
    "print(\"Columns with zero values:\\n\", zero_values)\n",
    "print(\"\\nColumns with negative values:\\n\", neg_values)\n",
    "\n",
    "# We can proceed with logarithm transformation on days_since_last_order and total_expenses\n",
    "log_df_rfm = np.log1p(df_rfm)\n",
    "\n",
    "# Add the remaining columns to the log_df_rfm dataframe\n",
    "# log_df_rfm[\"total_orders\"] = df_rfm[\"total_orders\"]\n",
    "\n",
    "# Check for NaN values after applying log1p\n",
    "nan_values = log_df_rfm.isna().sum()\n",
    "print(\"\\nColumns with NaN values after log1p:\\n\", nan_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the logarithm, let's normalize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distribution of the scaled features\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 12))\n",
    "\n",
    "for i, feature in enumerate(df_rfm.columns):\n",
    "    sns.histplot(df_rfm[feature], kde=True, ax=axes[i,0])\n",
    "    axes[i,0].set_title(f'{feature} original distribution')\n",
    "    axes[i,0].set_xlabel('Value')\n",
    "    axes[i,0].set_ylabel('Count')\n",
    "\n",
    "    # Scaled features\n",
    "    sns.histplot(scaled_df_rfm[feature], kde=True, ax=axes[i,1])\n",
    "    axes[i,1].set_title(f'{feature} scaled distribution')\n",
    "    axes[i,1].set_xlabel('Value')\n",
    "    axes[i,1].set_ylabel('Count')\n",
    "\n",
    "    # MinMax scaled features\n",
    "    sns.histplot(minmax_scaled_df_rfm[feature], kde=True, ax=axes[i,2])\n",
    "    axes[i,2].set_title(f'{feature} MinMax scaled distribution')\n",
    "    axes[i,2].set_xlabel('Value')\n",
    "    axes[i,2].set_ylabel('Count')\n",
    "\n",
    "    # Log distribution\n",
    "    sns.histplot(log_df_rfm[feature], kde=True, ax=axes[i,3])\n",
    "    axes[i,3].set_title(f'log_{feature} distribution')\n",
    "    axes[i,3].set_xlabel('Value')\n",
    "    axes[i,3].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log + normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaled_log_features = scaler.fit_transform(log_df_rfm)\n",
    "\n",
    "scaled_log_df_rfm = pd.DataFrame(\n",
    "    scaled_log_features, columns=df_rfm.columns\n",
    ")\n",
    "\n",
    "\n",
    "# MinMax scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaled_log_features = minmax_scaler.fit_transform(log_df_rfm)\n",
    "\n",
    "minmax_scaled_log_df_rfm = pd.DataFrame(\n",
    "    minmax_scaled_log_features, columns=df_rfm.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distribution of the scaled features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 12))\n",
    "\n",
    "for i, feature in enumerate(df_rfm.columns):\n",
    "    sns.histplot(df_rfm[feature], kde=True, ax=axes[i, 0])\n",
    "    axes[i, 0].set_title(f\"{feature} original distribution\")\n",
    "    axes[i, 0].set_xlabel(\"Value\")\n",
    "    axes[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    # Scaled features\n",
    "    sns.histplot(scaled_log_df_rfm[feature], kde=True, ax=axes[i, 1])\n",
    "    axes[i, 1].set_title(f\"{feature} scaled log distribution\")\n",
    "    axes[i, 1].set_xlabel(\"Value\")\n",
    "    axes[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "    # MinMax scaled features\n",
    "    sns.histplot(minmax_scaled_log_df_rfm[feature], kde=True, ax=axes[i, 2])\n",
    "    axes[i, 2].set_title(f\"{feature} MinMax scaled log distribution\")\n",
    "    axes[i, 2].set_xlabel(\"Value\")\n",
    "    axes[i, 2].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our project, we will choose to continue with the standard scaler normalizer. As we can observe, the distribution is gaussian and knowing that Kmeans algorithm is based on euclidian distance, Standard Scaler seems to be the most appropriate normalization after a logarithm transformation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_rfm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df_rfm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distorsion is the sum of squared distances from each point to its assigned center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2, 10), metric='distortion', timings=True)\n",
    "\n",
    "visualizer.fit(scaled_df_rfm)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2, 10))\n",
    "\n",
    "visualizer.fit(log_df_rfm)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log + Normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2, 10))\n",
    "\n",
    "visualizer.fit(scaled_log_df_rfm)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2, 10))\n",
    "\n",
    "visualizer.fit(minmax_scaled_log_df_rfm)\n",
    "visualizer.poof()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to evaluate the density and separation between clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in range(3, 5):\n",
    "    model = KMeans(n_clusters=n_clusters)\n",
    "    visualizer = SilhouetteVisualizer(model)\n",
    "\n",
    "    visualizer.fit(scaled_df_rfm)\n",
    "    visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in range(3, 7):\n",
    "    model = KMeans(n_clusters=n_clusters)\n",
    "    visualizer = SilhouetteVisualizer(model)\n",
    "\n",
    "    visualizer.fit(scaled_log_df_rfm)\n",
    "    visualizer.poof()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in range(3, 7):\n",
    "    model = KMeans(n_clusters=n_clusters)\n",
    "    visualizer = SilhouetteVisualizer(model)\n",
    "\n",
    "    visualizer.fit(minmax_scaled_log_df_rfm)\n",
    "    visualizer.poof()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters in range(3, 6):\n",
    "    model = KMeans(n_clusters=n_clusters)\n",
    "    visualizer = SilhouetteVisualizer(model)\n",
    "\n",
    "    visualizer.fit(log_df_rfm)\n",
    "    visualizer.poof()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler (Not used because better results with a logarithmic scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3, 5):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_std = df_rfm.copy(deep=True)\n",
    "    kmeans_std[\"cluster\"] = kmeans.fit_predict(scaled_df_rfm)\n",
    "\n",
    "    # Calculate the silhouette score\n",
    "    silhouette = silhouette_score(scaled_df_rfm, kmeans.labels_)\n",
    "    print(f\"Silhouette score for {k} clusters: {silhouette}\")\n",
    "\n",
    "    print(f\"Results for {k} clusters\")\n",
    "    benchmark_clusters(kmeans_std)\n",
    "    plot_3d_clusters(kmeans_std)\n",
    "    plot_radar_chart(kmeans_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3, 6):\n",
    "    kmean_log = df_rfm.copy(deep=True)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, max_iter=300)\n",
    "    kmean_log[\"cluster\"] = kmeans.fit_predict(scaled_log_df_rfm)\n",
    "\n",
    "    # Calculate the silhouette score\n",
    "    silhouette = silhouette_score(scaled_log_df_rfm, kmean_log[\"cluster\"])\n",
    "    print(f\"Silhouette average score for {k} clusters: {silhouette}\")\n",
    "\n",
    "    print(f\"Results for {k} clusters\")\n",
    "    log_df_rfm[\"cluster\"] = kmean_log[\"cluster\"]\n",
    "    plot_3d_clusters(log_df_rfm)\n",
    "    benchmark_clusters(kmean_log)\n",
    "    # plot_radar_chart(log_df_rfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minmax log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3, 5):\n",
    "    kmean_log = df_rfm.copy(deep=True)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, max_iter=300)\n",
    "    kmean_log[\"cluster\"] = kmeans.fit_predict(minmax_scaled_log_df_rfm)\n",
    "\n",
    "    # Calculate the silhouette score\n",
    "    silhouette = silhouette_score(minmax_scaled_log_df_rfm, kmean_log[\"cluster\"])\n",
    "    print(f\"Silhouette score for {k} clusters: {silhouette}\")\n",
    "\n",
    "    print(f\"Results for {k} clusters\")\n",
    "    plot_3d_clusters(log_df_rfm)\n",
    "    log_df_rfm[\"cluster\"] = kmean_log[\"cluster\"]\n",
    "    benchmark_clusters(kmean_log)\n",
    "    plot_radar_chart(log_df_rfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3, 6):\n",
    "    kmean_log = df_rfm.copy()\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, max_iter=300)\n",
    "    kmean_log[\"cluster\"] = kmeans.fit_predict(log_df_rfm)\n",
    "\n",
    "    # Calculate the silhouette score\n",
    "    silhouette = silhouette_score(log_df_rfm, kmean_log[\"cluster\"])\n",
    "    print(f\"Silhouette score for {k} clusters: {silhouette}\")\n",
    "\n",
    "    print(f\"Results for {k} clusters\")\n",
    "    log_df_rfm[\"cluster\"] = kmean_log[\"cluster\"]\n",
    "    benchmark_clusters(kmean_log)\n",
    "    plot_3d_clusters(log_df_rfm)\n",
    "    plot_radar_chart(log_df_rfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN takes several hyperparameters that can influence the results.\n",
    "- **eps** : maximum distance to consider two points as neighbors. It is the radius of the circle drew around the data.\n",
    "- **min_samples** : Minimum points that should be in a circle of the defined radius.\n",
    "\n",
    "There are 3 types of data points :\n",
    "- **Core point** : A point is a core point if it has more than MinPts points within eps.\n",
    "- **Border point** : A point which has fewer than MinPoints within eps but it is in the neighborhood of a core point.\n",
    "- **Noise** : A point which is not a core point or border point.\n",
    "\n",
    "\n",
    "Epsylon and min_samples help to fine-tune how DBSCAN works on the data. For this reason, we will try to evaluate the silhouette score with differents values for these two hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"esp\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"min_samples\": [3, 5, 7, 9, 10],\n",
    "}\n",
    "\n",
    "combinations = list(itertools.product(params_grid[\"esp\"], params_grid[\"min_samples\"]))\n",
    "\n",
    "print(f\"Total number of combinations: {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_rfm = log_df_rfm.copy(deep=True)\n",
    "dbscan_rfm = dbscan_rfm.drop(columns=[\"cluster\"])\n",
    "dbscan_rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search(combinations, dbscan_rfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best params to DBSCAN\n",
    "dbscan = DBSCAN(eps=best_params[\"epsylon\"], min_samples=best_params[\"min_samples\"])\n",
    "dbscan_rfm[\"cluster\"] = dbscan.fit_predict(dbscan_rfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters\n",
    "plot_3d_clusters(dbscan_rfm, \"DBSCAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster -1 means that these instances are considered as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_clusters(df_rfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hierarchical clustering approach is based on the determination of successive clusters based on previously defined clusters. It's a technique aimed more toward grouping data into a tree of clusters called dendrograms, which graphically represents the hierarchical relationship between the underlying clusters.\n",
    "\n",
    "The Agglomerative Hierarchical Clustering technique employs the use of distance measures to generate clusters. This generation process involves the following main steps: \n",
    "- Compute distance matrix (using distance metrics as Euclidian, Manhattan or Cosine)\n",
    "- Merge closest clusters\n",
    "- Update distance matrix\n",
    "- Repeat previous steps until only one cluster is left\n",
    "\n",
    "It exists different kind of hierarchical clustering :\n",
    "- *Agglomerative* : Starts considering each observation as a cluster with a unique datapoint then merge iteratively clusters untils there is only one left;\n",
    "- *Divisive* : Starts considering all datapoints as a unique cluster then seperates them until all datapoints become unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 10% of the dataset\n",
    "sample = log_df_rfm.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# scaling\n",
    "scaler = MinMaxScaler()\n",
    "sample_scaled = scaler.fit_transform(sample)\n",
    "\n",
    "# Shot list of distances measures methods\n",
    "distance_measures = [\"euclidean\", \"cityblock\", \"cosine\"]\n",
    "# Differents linkage methods\n",
    "linkage_methods = [\"average\", \"complete\", \"single\", \"ward\", \"centroid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_best_params = get_ahc_best_params(sample_scaled, distance_measures, linkage_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_best_params = (\"euclidean\", \"centroid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(sample_scaled, method=sample_best_params[1], metric=sample_best_params[0])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.title(f'Hierarchical Clustering Dendrogram using {sample_best_params[1]} linkage method and {sample_best_params[0]} distance metric')\n",
    "dendrogram(Z)\n",
    "\n",
    "plt.xlabel('Sample indexes')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linkage matrix Z, each row represents a merge between two clusters, and the columns contain specific information about these merges:\n",
    "\n",
    "- Column 0: The first cluster being merged.\n",
    "- Column 1: The second cluster being merged.\n",
    "- Column 2: The distance between the two clusters being merged.\n",
    "- Column 3: The number of original observations in the newly formed cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best treshold to define number of classes\n",
    "best_score = -1\n",
    "max_clusters = 10\n",
    "\n",
    "for t in np.linspace(0, np.max(Z[:, 2]), 100):\n",
    "    clusters = fcluster(Z, t, criterion='distance')\n",
    "    if len(set(clusters)) > 1 and len(set(clusters)) <= max_clusters:\n",
    "        score = silhouette_score(sample, clusters)\n",
    "        if score > best_score:\n",
    "            print(score, t)\n",
    "            best_score = score\n",
    "            best_threshold = t\n",
    "\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\n",
    "    f\"Hierarchical Clustering Dendrogram using {sample_best_params[1]} linkage method and {sample_best_params[0]} distance metric\"\n",
    ")\n",
    "dendrogram(Z, color_threshold=21)\n",
    "plt.axhline(y=21, color=\"r\", linestyle=\"--\")\n",
    "plt.xlabel(\"Sample indexes\")\n",
    "plt.ylabel(\"Distance\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(fcluster(Z, 21, criterion='distance')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "full_data_best_params = get_ahc_best_params(log_df_rfm, distance_measures, linkage_methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing our clustering algorithm, we chose to continue with K-means algoruthm. \n",
    "\n",
    "The hierarichal clustering is too demanding in computing performance and DBscan results gave us less precised clusters.\n",
    "\n",
    "Let's now add few features to our RFM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_rfm to pickle\n",
    "df_rfm.to_pickle('../../data/base_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
